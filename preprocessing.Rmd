---
title: "Predicting Law Enforcement Intervention in Protests"
subtitle: "GOV 2018 Final Project"
author: "Alexandra Norris and Angie Shin"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)

acled <- read.csv("data/acled.csv")
load("data/hansonsigman.RData")
hs <- table
rm(table)

# table(is.na(acled) == TRUE)
# 
# table(acled$sub_event_type)
# table(acled$source_scale)
# table(acled$time_precision)
# table(acled$geo_precision)
# table(acled$region)
# table(acled$inter1)
# 
# table(acled$sub_event_type, acled$inter2)
# 
# summary(acled$year)
# summary(acled$fatalities)
# summary(acled$interaction)
# 
# names(acled)
names(hs)
```

## Preprocessing & Planning Instructions

Conduct preprocessing of the data using clean, replicable code with informative comments for readers to effortlessly read and learn from. Explain clearly (so colleagues can understand you) why your project’s chosen ML approach (or approaches) is reasonable for this application (pros and cons) as well as the assumptions made in using the approach and why it is a better approach to use than possible suggested alternatives.

Deliverables:
* Selected ML strategy (or strategies)
* Preprocessing and cleaning code; cleaned data ready for analysis

# ML Strategy

Why are our project’s chosen ML approaches reasonable for this application?
Our research question is to predict law enforcement intervention in protests, so we are using the 500k+-row ACLED dataset of protests around the world as well as the Hanson and Sigman dataset of state capacity covariates (traditionally used in comparative politics). Put together, these datasets will hopefully provide us with enough observations and variables to predict whether or not law enforcement will intervene in protests. For our purposes, therefore, we will use logistic regression, k-Nearest Neighbors, decision trees, and Support Vector Machines as our selected machine learning approaches—these classification methods will help us get a feel for what approaches are the most accurate for our data.

What are the assumptions made in using these approaches?


Why are these approaches better to use than other alternatives?


# Cleaning Data

```{r clean}
acledm <- acled %>%
  select(iso3,
         ptype = sub_event_type, fatalities,
         timep = time_precision, 
         date = event_date, year, 
         geop = geo_precision, region, country, location, latitude, longitude,
         actor1, assoc1 = assoc_actor_1, inter1, 
         actor2, assoc2 = assoc_actor_2, inter2, interaction,
         admin1, admin2, admin3, 
         source, source_scale, notes) %>% 
  filter(year >= 2000) %>% 
  mutate(date = as.Date(date, "%d %b %Y"))

hsm <- hs %>% 
  select(-c(cntrynum, country, year, iso2, ccode, scode))

protests <- left_join(acledm, hsm, by = "iso3")

write.csv(protests, file = "data/protests.csv")
```

